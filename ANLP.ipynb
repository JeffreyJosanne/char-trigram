{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "Here, we have used regular expressions to remove the unwanted characters instead of a for loop, which will reduce the number of computations made while removing such characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##resumption of the session#\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re, os, pygtrie, numbers\n",
    "dimension = 30\n",
    "def readFile(path):\n",
    "    list = []\n",
    "    input = open(path, 'r')\n",
    "    list.append(input.readlines())\n",
    "    return(list)\n",
    "def process_line(string):\n",
    "    string = re.sub(r'[^A-Za-z0-9\\ \\.]', \"\", string)\n",
    "    string = re.sub('[0-9]', \"0\",string)\n",
    "    string = string.lower()\n",
    "    string = \"##\" + string + \"#\"\n",
    "    return(string)\n",
    "processed_lines = []\n",
    "list = readFile(\"training.en\")\n",
    "for l in list:\n",
    "    for line in l:\n",
    "        processed_lines.append(process_line(line))\n",
    "#     print('-------------------')\n",
    "# print(processed_lines)\n",
    "# for processed_line in processed_lines:\n",
    "print(processed_lines[0])\n",
    "\n",
    "triGram = np.zeros((30,30,30))\n",
    "# print(processed_lines)\n",
    "for processed_line in processed_lines:\n",
    "    constructTriGram(processed_line.strip(' ')) # shift it from the for above\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge function:\n",
    "Bridging function to function for entering trigram sequences from line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constructTriGram(line):\n",
    "    length = len(line)\n",
    "    listOfGrams = []\n",
    "    for i in range(length):\n",
    "        c = []\n",
    "        if i is 0 or i is 1:\n",
    "            continue\n",
    "        else:\n",
    "            enterSequence(line[i-2:i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char2matrix function:\n",
    "characters are being converted to ascii values and scaled within 29 so that these values can be used as indices for the trigram and tricount matrix. Certain unviable sequences are restricted to pass through the loops for counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enterSequence(sequence):\n",
    "    seq=[]\n",
    "    seq.append(ord(sequence[0])%97)\n",
    "    seq.append(ord(sequence[1])%97)\n",
    "    seq.append(ord(sequence[2])%97)\n",
    "    seq = getascii(seq)\n",
    "    i = seq[0]\n",
    "    j = seq[1]\n",
    "    k = seq[2]\n",
    "    if ((i == 29 and j == 29 and k == 29)):\n",
    "        print('###')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (j == 29 and k == 29):\n",
    "        print('c##')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (i == 29 and k == 29):\n",
    "        print('#c#')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (i == 29 and k == 29 and j!=29):\n",
    "        print('#c#')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (i != 29 and j == 29 and k!=29):\n",
    "        print('c#c')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    else:\n",
    "        triGram[i][j][k] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ascii conversion tools (trigram & character):\n",
    "ascii values are divided by 97 which is the ascii value of 'a' to scale all down within 0 to 26 for alphabets and other characters scaled down appropriately.\n",
    "\n",
    "Decoding algorithm is at the bottom of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getascii(seq):\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] == 32:\n",
    "            seq[i] = 26\n",
    "        if seq[i] == 46:\n",
    "            seq[i] = 27\n",
    "        if seq[i] == 48:\n",
    "            seq[i] = 28\n",
    "        if seq[i] == 35:\n",
    "            seq[i] = 29\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getasciiForSingleChar(character):\n",
    "    char = ord(character)%97\n",
    "    if char == 32:\n",
    "        char = 26\n",
    "    if char == 46:\n",
    "        char = 27\n",
    "    if char == 48:\n",
    "        char = 28\n",
    "    if char == 35:\n",
    "        char = 29\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asciiToChars(value):\n",
    "    char =''\n",
    "    if value < 26:\n",
    "        char = chr(int(value+97))\n",
    "    elif value == 26:\n",
    "        char = ' '\n",
    "    elif value == 27:\n",
    "        char = '.'\n",
    "    elif value == 28:\n",
    "        char = '0'\n",
    "    elif value == 29:\n",
    "        char = '#'\n",
    "    return char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grams for readability.\n",
    "Bigrams and unigrams matrices are made for readability. Actually matrices are chosen in this assignment so that bigrams and unigrams can be derived from the trigram matrix itself. This strategy is used having the space complexity in mind. This will reduce (vocabulary * vocabulary + vocabulary) memory blocks.\n",
    "\n",
    "For instance:\n",
    "bi_gram[char1][char2] = sum(tri_gram[char1][char2])\n",
    "The idea is to not use these exclusive bi_gram or uni_gram matrices ,rather we can use tri_gram matrices itself to derive the other matrices without allocating any memory to those. Matrices are chose over dictionaries and hashmaps for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBigram(freq_matrix):\n",
    "    bi_grams = np.zeros((dimension,dimension))\n",
    "    for i in range(dimension):\n",
    "        for j in range(dimension):\n",
    "            bi_grams[i][j] = sum(freq_matrix[i][j])\n",
    "    return(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUnigram(bi_grams):\n",
    "    uni_grams = np.zeros((dimension))\n",
    "    for i in range(dimension):\n",
    "        uni_grams[i] = sum(bi_grams[i])\n",
    "    return(uni_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split for training and test data.\n",
    "80% of the data is used for tarining and final 20% is used as developmental set to find alpha values if Good turing was used. To get hold of the context of the corpus all over, we are stratifying (juggling) the data to get the sentence of every part of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainAndTestData(corpus, dev_part):\n",
    "    length = len(corpus)\n",
    "    folded_data = []\n",
    "    folds = 5\n",
    "    fold = int(length/5)\n",
    "    start = 0\n",
    "    print(fold)\n",
    "    for i in range(len(corpus)):\n",
    "        if (i%fold == 0):\n",
    "            folded_data.append(corpus[start:i])\n",
    "            start = i\n",
    "    print(folded_data)\n",
    "    dev = folded_data[dev_part]\n",
    "    train = folded_data.pop(dev_part)\n",
    "    print(dev)\n",
    "    print('---')\n",
    "    print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['', 'Here w', 'e go, ', 'I am j', 'ust ch']\n",
      "Here w\n",
      "---\n",
      "Here w\n"
     ]
    }
   ],
   "source": [
    "getTrainAndTestData(\"Here we go, I am just checking\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def getStratifiedSamplingOfData(corpus, dev_part):\n",
    "    sent_list = sent_tokenize(corpus)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for i in range(len(sent_list)):\n",
    "        if i%5 == 0:\n",
    "            test_data.append(sent_list[i])\n",
    "        else:\n",
    "            train_data.append(sent_list[i])\n",
    "    print(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is sent two.', 'is this sent three?', 'sent 4 is cool!', 'Now it’s your turn.']\n"
     ]
    }
   ],
   "source": [
    "whichPart = 0\n",
    "corpus = \"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
    "getStratifiedSamplingOfData(corpus, whichPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add alpha smoothing:\n",
    "pass 1 as the parameter of 'function generateAddOneSmoothing()' to get add 1 smoothing. 0 < alpha < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateAddOneSmoothing(alpha):\n",
    "    tri_gram_add_one = np.zeros((dimension, dimension, dimension))\n",
    "    for w1 in range(dimension):\n",
    "        for w2 in range(dimension):\n",
    "            for w3 in range(dimension):\n",
    "                tri_gram_add_one[w1][w2][w3] = ((triGram[w1][w2][w3] + alpha) / (sum(triGram[w1][w2]) + (alpha*(dimension))))\n",
    "    return(tri_gram_add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tri_gram_one = generateAddOneSmoothing(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARPA creation:\n",
    "Craete ARPA format of the language model to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def createArpa(): \n",
    "    file = open(\"trigram_add_one.en\",\"w\")\n",
    "    for i in range(30):\n",
    "        for j in range(30):\n",
    "            for k in range(30):\n",
    "                if ((i == 29 and j == 29 and k == 29)):\n",
    "#                     print('###')\n",
    "#                     print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "                    continue\n",
    "                elif (j == 29 and k == 29):\n",
    "#                     print('c##')\n",
    "#                     print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "                    continue\n",
    "                elif (i != 29 and j == 29 and k!=29):\n",
    "#                     print('c#c')\n",
    "#                     print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "                    continue\n",
    "                else:\n",
    "                    s = \"\"\n",
    "                    listSeq = []\n",
    "                    listSeq.append(asciiToChars(i))\n",
    "                    listSeq.append(asciiToChars(j))\n",
    "                    listSeq.append(asciiToChars(k))\n",
    "                    listSeq.append(\"\\t\")\n",
    "                    listSeq.append(str(format_e(tri_gram_one[i][j][k])))\n",
    "#                     print(tri_gram_one[i][j][k])\n",
    "                    listSeq.append(\"\\n\")\n",
    "                    file.write(s.join(listSeq)) \n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createArpa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation algorithm:\n",
    "To avoid overfitting, we are considering the top 2 values for every possible sequence. So that we dont get the same sequence as we got in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "theText = []\n",
    "def generateText(from_where):\n",
    "    temp = 0\n",
    "    first = 29\n",
    "    second = 29\n",
    "    theOne = 0\n",
    "    for i in range(300):\n",
    "        theOne = randint(0,2)\n",
    "        if from_where == 'add_one':\n",
    "            theOne = randint(0,2)\n",
    "            probableChar = sorted(range(len(tri_gram_one[first][second])), key=lambda i: tri_gram_one[first][second][i], reverse=True)[:3]\n",
    "        if from_where == 'from_arpa':\n",
    "            theOne = randint(0,2)\n",
    "            probableChar = sorted(range(len(tri_gram_from_arpa[first][second])), key=lambda i: tri_gram_from_arpa[first][second][i], reverse=True)[:3]\n",
    "        if from_where == 'given_model_1':\n",
    "            theOne = 0\n",
    "            probableChar = sorted(range(len(a[first][second])), key=lambda i: a[first][second][i], reverse=True)[:3]\n",
    "        theText.append((probableChar[theOne]))\n",
    "        temp = (probableChar[theOne])\n",
    "        first = second\n",
    "        second = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateAlgo(from_where):\n",
    "    generateText(from_where) \n",
    "    text = ' '\n",
    "    for i in range(len(theText)):\n",
    "        text = text + asciiToChars(theText[i])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " theresee dad in to yon yoll.#clocl thaing having tay.aaack in ye ton.baby.balich thavin take.#alit ant to yaway whaill ing on.babyeat is tay.ball rinkey he daysee.bboor.aboy.#by.aabble bung talkink yaaby done.bcl thiss.balk.#bund youres takeyeats an thesee ther buir.bace don.bbloodad to yones.#a but\n"
     ]
    }
   ],
   "source": [
    "generateAlgo('from_arpa') # from_arpa # add_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to bus inte truct trult is ands#cal pands.aabin oustakilinte a pans an a pose clembe cating.#calleastakis an oft ofte catingeregregallemempectint trulthint oulan oner struct ouroge ased.bbbc paregivatin to an thint ouregregiourogropment of that tores thingen a sed ans trall ther as ithised an onspos\n"
     ]
    }
   ],
   "source": [
    "generateAlgo('add_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " babababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababababa\n"
     ]
    }
   ],
   "source": [
    "generateAlgo('given_model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser ney smoothing\n",
    "Kneser ney smoothing actually claimed to be the best language model (except KenLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateKNTriGram():\n",
    "    # D = [0.70, 0.20, 0.10]\n",
    "    D = 0.75\n",
    "    charCounts = dimension\n",
    "    bi_gram_count = getBigram(triGram)\n",
    "    uni_gram_count = getUnigram(bi_gram_count)\n",
    "    uni_gram = (uni_gram_count / sum(uni_gram_count))\n",
    "    bi_gram = np.zeros((dimension,dimension))\n",
    "    tri_gram = np.zeros((dimension,dimension,dimension))\n",
    "    for i in range(dimension):\n",
    "        for j in range(dimension):\n",
    "            if bi_gram_count[i][j] > 0:\n",
    "                bi_gram[i][j] = ( (bi_gram_count[i][j] - D) / uni_gram_count[i] ) + (((D * charCounts) / uni_gram_count[i]) * uni_gram[j])\n",
    "            else:\n",
    "                bi_gram[i][j] = (((D * charCounts) / uni_gram_count[i]) * uni_gram[j])\n",
    "                # print(bi_gram)\n",
    "    for w1 in range(dimension):\n",
    "        for w2 in range(dimension):\n",
    "            for w3 in range(dimension):\n",
    "                if triGram[w1][w2][w3] > 0:\n",
    "                    tri_gram[w1][w2][w3] = ((triGram[w1][w2][w3] - D) / bi_gram_count[w1][w2]) + ((D * charCounts) / bi_gram_count[w1][w2]) * bi_gram[w2][w3]\n",
    "                elif bi_gram_count[w1][w2] > 0:\n",
    "                    tri_gram[w1][w2][w3] = ((D * charCounts) / bi_gram_count[w1][w2]) * bi_gram[w2][w3]\n",
    "#                 else:\n",
    "#                     tri_gram[w1][w2][w3] = \n",
    "                    ##################### Work in pending #####################\n",
    "    print(tri_gram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network:\n",
    "Helper algorithms are in the first cell. Second cell has training and generation model. word2vec vectors are used and the number of epochs are set as 33. This number seemed to have a better generation and to avoid moving further to prevent overfitting. Keras is used to complete the model faster.\n",
    "Sigmoid is used for activation and the movinng average is always kept over the root mean square hence the RMSprop algorithm. Adam can also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)\n",
    "\n",
    "\n",
    "def load_data(data_dir, seq_length):\n",
    "    data = open(data_dir, 'r').read()\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "\n",
    "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "    X = np.zeros((int(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((int(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
    "    for i in range(0, int(len(data)/seq_length)):\n",
    "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "\n",
    "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "    return X, y, VOCAB_SIZE, ix_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "HIDDEN_DIM = 500\n",
    "SEQ_LENGTH = 50\n",
    "WEIGHTS = ''\n",
    "mode = 'train'\n",
    "GENERATE_LENGTH = 500\n",
    "LAYER_NUM = 2\n",
    "\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data('assignment1-data/training.en', SEQ_LENGTH)\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "  model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "generate_text(model, 500, VOCAB_SIZE, ix_to_char)\n",
    "if not WEIGHTS == '':\n",
    "  model.load_weights(WEIGHTS)\n",
    "  nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "else:\n",
    "  nb_epoch = 0\n",
    "if mode == 'train' or WEIGHTS == '':\n",
    "  while nb_epoch<33:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    if nb_epoch % 10 == 0:\n",
    "      model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "\n",
    "elif WEIGHTS == '':\n",
    "  model.load_weights(WEIGHTS)\n",
    "  generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "  print('\\n\\n')\n",
    "else:\n",
    "  print('\\n\\nNothing to do!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(lines, model=''):\n",
    "    doc_length = 0\n",
    "    total_log_sum = 0\n",
    "    probability = 0\n",
    "    perplexity = 0\n",
    "    line_perplexities = []\n",
    "    for line in lines:\n",
    "#         line_perplexity = 0\n",
    "        for char_index in range(len(line) - 2):\n",
    "            probability = tri_gram_one[getasciiForSingleChar(line[char_index])][getasciiForSingleChar(line[char_index+1])][getasciiForSingleChar(line[char_index+2])]\n",
    "#             probability = a[getasciiForSingleCharForGiven(line[char_index])][getasciiForSingleCharForGiven(line[char_index+1])][getasciiForSingleCharForGiven(line[char_index+2])]        \n",
    "            doc_length += 1\n",
    "            perplexity += math.log(probability, 2)\n",
    "    perplexity = perplexity / doc_length\n",
    "    perplexity = math.pow(2, -perplexity)\n",
    "#         print(perplexity)\n",
    "#             if probability > 0:\n",
    "#                 probability = np.log2(probability)\n",
    "#             line_perplexity += probability\n",
    "#         line_perplexities.append(line_perplexity)\n",
    "#     for line_perplexity in line_perplexities:\n",
    "#         total_log_sum += line_perplexity\n",
    "#     perplexity = 2 ** -((doc_length/(total_log_sum)))\n",
    "    print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.86952152144219\n"
     ]
    }
   ],
   "source": [
    "processed_test_lines = []\n",
    "list = readFile(\"test\")\n",
    "for l in list:\n",
    "    for line in l:\n",
    "        processed_test_lines.append(process_line(line))\n",
    "perplexity(processed_test_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARPA2matrix:\n",
    "Converting from ARPA to matrix format so that lookup is faster than the arpa format or any other data structures. All the models must be converted to matrix format for computaional comparison. For visual comparison, ARPA should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arpa2matrix():\n",
    "    tri_gram = np.zeros((30,30,30))\n",
    "    arpa_format = open('model-br.en', 'r').readlines()\n",
    "    for row in arpa_format:\n",
    "        sequence = (row[0:3])\n",
    "        score = float(row[4:13])\n",
    "        tri_gram[getasciiForSingleChar(sequence[0])][getasciiForSingleChar(sequence[1])][getasciiForSingleChar(sequence[2])] = score\n",
    "    return tri_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  4.20200000e-04   9.70600000e-02   4.24400000e-02 ...,   4.20200000e-04\n",
      "     4.20200000e-04   4.20200000e-04]\n",
      "  [  5.75900000e-03   5.23600000e-04   5.75900000e-03 ...,   5.23600000e-04\n",
      "     5.23600000e-04   5.23600000e-04]\n",
      "  ..., \n",
      "  [  8.85000000e-04   8.85000000e-04   8.85000000e-04 ...,   8.85000000e-04\n",
      "     8.85000000e-04   9.74300000e-01]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  3.95300000e-04   2.45500000e-01   3.60100000e-01 ...,   3.95300000e-04\n",
      "     3.95300000e-04   3.95300000e-04]\n",
      "  [  3.12500000e-03   3.12500000e-03   3.12500000e-03 ...,   3.12500000e-03\n",
      "     3.12500000e-03   3.12500000e-03]\n",
      "  [  7.69200000e-03   7.69200000e-03   7.69200000e-03 ...,   7.69200000e-03\n",
      "     7.69200000e-03   7.69200000e-03]\n",
      "  ..., \n",
      "  [  4.00000000e-03   4.00000000e-03   4.00000000e-03 ...,   4.00000000e-03\n",
      "     4.00000000e-03   8.84000000e-01]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  1.62300000e-04   1.62300000e-04   1.62300000e-04 ...,   1.62300000e-04\n",
      "     1.62300000e-04   1.62300000e-04]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  2.50000000e-02   2.50000000e-02   2.50000000e-02 ...,   2.50000000e-02\n",
      "     2.50000000e-02   2.50000000e-02]\n",
      "  ..., \n",
      "  [  1.42900000e-02   1.42900000e-02   1.42900000e-02 ...,   1.42900000e-02\n",
      "     1.42900000e-02   5.85700000e-01]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " ..., \n",
      " [[  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  ..., \n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  ..., \n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  3.33300000e-02   3.33300000e-02   3.33300000e-02 ...,   3.33300000e-02\n",
      "     3.33300000e-02   3.33300000e-02]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  1.51100000e-04   6.19400000e-03   1.51100000e-04 ...,   1.66200000e-03\n",
      "     1.51100000e-04   0.00000000e+00]\n",
      "  [  1.50600000e-01   3.66400000e-04   3.66400000e-04 ...,   3.66400000e-04\n",
      "     3.66400000e-04   0.00000000e+00]\n",
      "  [  6.72200000e-01   2.54500000e-04   2.54500000e-04 ...,   2.54500000e-04\n",
      "     2.54500000e-04   0.00000000e+00]\n",
      "  ..., \n",
      "  [  3.44800000e-02   3.44800000e-02   3.44800000e-02 ...,   3.44800000e-02\n",
      "     3.44800000e-02   0.00000000e+00]\n",
      "  [  3.44800000e-02   3.44800000e-02   3.44800000e-02 ...,   3.44800000e-02\n",
      "     3.44800000e-02   0.00000000e+00]\n",
      "  [  6.73000000e-02   2.75800000e-02   3.98300000e-02 ...,   1.02100000e-05\n",
      "     1.02100000e-05   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "tri_gram_from_arpa = arpa2matrix()\n",
    "print(tri_gram_from_arpa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Issues with smoothing algorithms:\n",
    "1. They are not worried about long distant relationships between the characters. For example certain characters (wn) might be conditional on precedinng characters (like wn-3 and wn-5) skipping few grams.\n",
    "2. It does not carry forward any context variable for a specific sentence or a paragraph.\n",
    "3. In words language models, the words are not semantically or contextually clustered. This has been resolved by word2vec vectors. Creating a language model over these vectors in a might give us lot of information about a sentence while generating sequences from the model.\n",
    "4. Also on multiple iterations, the LSTM model seems to have promising text generation. But the training is time consuming. But the model outputs perfect english text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,3,3))\n",
    "a[0][0][0] = 0.4\n",
    "a[0][0][1] = 0.5\n",
    "a[0][0][2] = 0.1\n",
    "a[0][1][0] = 0.6\n",
    "a[0][1][1] = 0.3\n",
    "a[0][1][2] = 0.1\n",
    "a[1][1][0] = 0.5\n",
    "a[1][1][1] = 0.4\n",
    "a[1][1][2] = 0.1\n",
    "a[1][0][0] = 0.25\n",
    "a[1][0][1] = 0.65\n",
    "a[1][0][2] = 0.1\n",
    "a[2][2][0] = 0.2\n",
    "a[2][2][1] = 0.8\n",
    "a[2][2][2] = 0.0\n",
    "a[2][0][0] = 0.2\n",
    "a[2][0][1] = 0.7\n",
    "a[2][0][2] = 0.1\n",
    "a[2][1][0] = 0.15\n",
    "a[2][1][1] = 0.75\n",
    "a[2][1][2] = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3219280948873622"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(0.2)+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getasciiForSingleCharForGiven(character):\n",
    "    char = ord(character)%97\n",
    "    if char == 35:\n",
    "        char = 2\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
