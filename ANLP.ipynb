{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "Here, we have used regular expressions to remove the unwanted characters instead of a for loop, which will reduce the number of computations made while removing such characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##resumption of the session#\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re, os, pygtrie, numbers\n",
    "dimension = 30\n",
    "def readFile(path):\n",
    "    list = []\n",
    "    input = open(path, 'r')\n",
    "    list.append(input.readlines())\n",
    "    return(list)\n",
    "def process_line(string):\n",
    "    string = re.sub(r'[^A-Za-z0-9\\ \\.]', \"\", string)\n",
    "    string = re.sub('[0-9]', \"0\",string)\n",
    "    string = string.lower()\n",
    "    string = \"##\" + string + \"#\"\n",
    "    return(string)\n",
    "processed_lines = []\n",
    "list = readFile(\"assignment1-data/training.en\")\n",
    "for l in list:\n",
    "    for line in l:\n",
    "        processed_lines.append(process_line(line))\n",
    "#     print('-------------------')\n",
    "# print(processed_lines)\n",
    "# for processed_line in processed_lines:\n",
    "print(processed_lines[0])\n",
    "\n",
    "triGram = np.zeros((30,30,30))\n",
    "# print(processed_lines)\n",
    "for processed_line in processed_lines:\n",
    "    constructTriGram(processed_line.strip(' ')) # shift it from the for above\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge function:\n",
    "Bridging function to function for entering trigram sequences from line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constructTriGram(line):\n",
    "    length = len(line)\n",
    "    listOfGrams = []\n",
    "    for i in range(length):\n",
    "        c = []\n",
    "        if i is 0 or i is 1:\n",
    "            continue\n",
    "        else:\n",
    "            enterSequence(line[i-2:i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char2matrix function:\n",
    "characters are being converted to ascii values and scaled within 29 so that these values can be used as indices for the trigram and tricount matrix. Certain unviable sequences are restricted to pass through the loops for counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enterSequence(sequence):\n",
    "    seq=[]\n",
    "    seq.append(ord(sequence[0])%97)\n",
    "    seq.append(ord(sequence[1])%97)\n",
    "    seq.append(ord(sequence[2])%97)\n",
    "    seq = getascii(seq)\n",
    "    i = seq[0]\n",
    "    j = seq[1]\n",
    "    k = seq[2]\n",
    "    if ((i == 29 and j == 29 and k == 29)):\n",
    "        print('###')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (j == 29 and k == 29):\n",
    "        print('c##')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (i == 29 and k == 29):\n",
    "        print('#c#')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (i == 29 and k == 29 and j!=29):\n",
    "        print('#c#')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    elif (i != 29 and j == 29 and k!=29):\n",
    "        print('c#c')\n",
    "        print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "    else:\n",
    "        triGram[i][j][k] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ascii conversion tools (trigram & character):\n",
    "ascii values are divided by 97 which is the ascii value of 'a' to scale all down within 0 to 26 for alphabets and other characters scaled down appropriately.\n",
    "\n",
    "Decoding algorithm is at the bottom of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getascii(seq):\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] == 32:\n",
    "            seq[i] = 26\n",
    "        if seq[i] == 46:\n",
    "            seq[i] = 27\n",
    "        if seq[i] == 48:\n",
    "            seq[i] = 28\n",
    "        if seq[i] == 35:\n",
    "            seq[i] = 29\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getasciiForSingleChar(character):\n",
    "    char = ord(character)%97\n",
    "    if char == 32:\n",
    "        char = 26\n",
    "    if char == 46:\n",
    "        char = 27\n",
    "    if char == 48:\n",
    "        char = 28\n",
    "    if char == 35:\n",
    "        char = 29\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asciiToChars(value):\n",
    "    char =''\n",
    "    if value < 26:\n",
    "        char = chr(int(value+97))\n",
    "    elif value == 26:\n",
    "        char = ' '\n",
    "    elif value == 27:\n",
    "        char = '.'\n",
    "    elif value == 28:\n",
    "        char = '0'\n",
    "    elif value == 29:\n",
    "        char = '#'\n",
    "    return char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grams for readability.\n",
    "Bigrams and unigrams matrices are made for readability. Actually matrices are chosen in this assignment so that bigrams and unigrams can be derived from the trigram matrix itself. This strategy is used having the space complexity in mind. This will reduce (vocabulary * vocabulary + vocabulary) memory blocks.\n",
    "\n",
    "For instance:\n",
    "bi_gram[char1][char2] = sum(tri_gram[char1][char2])\n",
    "The idea is to not use these exclusive bi_gram or uni_gram matrices ,rather we can use tri_gram matrices itself to derive the other matrices without allocating any memory to those. Matrices are chose over dictionaries and hashmaps for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBigram(freq_matrix):\n",
    "    bi_grams = np.zeros((dimension,dimension))\n",
    "    for i in range(dimension):\n",
    "        for j in range(dimension):\n",
    "            bi_grams[i][j] = sum(freq_matrix[i][j])\n",
    "    return(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUnigram(bi_grams):\n",
    "    uni_grams = np.zeros((dimension))\n",
    "    for i in range(dimension):\n",
    "        uni_grams[i] = sum(bi_grams[i])\n",
    "    return(uni_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split for training and test data.\n",
    "80% of the data is used for tarining and final 20% is used as developmental set to find alpha values if Good turing was used. To get hold of the context of the corpus all over, we are stratifying (juggling) the data to get the sentence of every part of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainAndTestData(corpus, dev_part):\n",
    "    length = len(corpus)\n",
    "    folded_data = []\n",
    "    folds = 5\n",
    "    fold = int(length/5)\n",
    "    start = 0\n",
    "    print(fold)\n",
    "    for i in range(len(corpus)):\n",
    "        if (i%fold == 0):\n",
    "            folded_data.append(corpus[start:i])\n",
    "            start = i\n",
    "    print(folded_data)\n",
    "    dev = folded_data[dev_part]\n",
    "    train = folded_data.pop(dev_part)\n",
    "    print(dev)\n",
    "    print('---')\n",
    "    print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['', 'Here w', 'e go, ', 'I am j', 'ust ch']\n",
      "Here w\n",
      "---\n",
      "Here w\n"
     ]
    }
   ],
   "source": [
    "getTrainAndTestData(\"Here we go, I am just checking\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def getStratifiedSamplingOfData(corpus, dev_part):\n",
    "    sent_list = sent_tokenize(corpus)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for i in range(len(sent_list)):\n",
    "        if i%5 == 0:\n",
    "            test_data.append(sent_list[i])\n",
    "        else:\n",
    "            train_data.append(sent_list[i])\n",
    "    print(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is sent two.', 'is this sent three?', 'sent 4 is cool!', 'Now it’s your turn.']\n"
     ]
    }
   ],
   "source": [
    "whichPart = 0\n",
    "corpus = \"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
    "getStratifiedSamplingOfData(corpus, whichPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add alpha smoothing:\n",
    "pass 1 as the parameter of 'function generateAddOneSmoothing()' to get add 1 smoothing. 0 < alpha < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateAddOneSmoothing(alpha):\n",
    "    tri_gram_add_one = np.zeros((dimension, dimension, dimension))\n",
    "    for w1 in range(dimension):\n",
    "        for w2 in range(dimension):\n",
    "            for w3 in range(dimension):\n",
    "                tri_gram_add_one[w1][w2][w3] = ((triGram[w1][w2][w3] + alpha) / (sum(triGram[w1][w2]) + (alpha*(dimension))))\n",
    "    return(tri_gram_add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tri_gram_one = generateAddOneSmoothing(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARPA creation:\n",
    "Craete ARPA format of the language model to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def createArpa(): \n",
    "    file = open(\"trigram_add_one.en\",\"w\")\n",
    "    for i in range(30):\n",
    "        for j in range(30):\n",
    "            for k in range(30):\n",
    "                if ((i == 29 and j == 29 and k == 29)):\n",
    "#                     print('###')\n",
    "#                     print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "                    continue\n",
    "                elif (j == 29 and k == 29):\n",
    "#                     print('c##')\n",
    "#                     print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "                    continue\n",
    "                elif (i != 29 and j == 29 and k!=29):\n",
    "#                     print('c#c')\n",
    "#                     print(asciiToChars(i)+asciiToChars(j)+asciiToChars(k))\n",
    "                    continue\n",
    "                else:\n",
    "                    s = \"\"\n",
    "                    listSeq = []\n",
    "                    listSeq.append(asciiToChars(i))\n",
    "                    listSeq.append(asciiToChars(j))\n",
    "                    listSeq.append(asciiToChars(k))\n",
    "                    listSeq.append(\"\\t\")\n",
    "                    listSeq.append(str(format_e(tri_gram_one[i][j][k])))\n",
    "#                     print(tri_gram_one[i][j][k])\n",
    "                    listSeq.append(\"\\n\")\n",
    "                    file.write(s.join(listSeq)) \n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "createArpa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation algorithm:\n",
    "To avoid overfitting, we are considering the top 2 values for every possible sequence. So that we dont get the same sequence as we got in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ccoures isenter a fuldinted is asent to bybbcatint oneregratiod a sen one tor to astake clan ouregrourouroultunise trinted.bat iss ase clan tring.#buropmad and a posent int traters it trioduchase astrioulans ansit trallempoleast oft it offor tratint is trulds.accus isse cludirs to bused astanspole coure tor as astruct in ould in ands thas ans tor se anspan thing to thesithis ingenteres ans the can ous a sends a sent oust is thativelin on trinte clustrultuatious isenden an as isedused.bcbalst tri\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "theText = []\n",
    "def generateText():\n",
    "    temp = 0\n",
    "    first = 0\n",
    "    second = 0\n",
    "    theOne = 0\n",
    "    for i in range(500):\n",
    "        theOne = randint(0,2)\n",
    "        probableChar = sorted(range(len(tri_gram_add_one[first][second])), key=lambda i: tri_gram_add_one[first][second][i], reverse=True)[:3]\n",
    "        theText.append((probableChar[theOne]))\n",
    "        temp = (probableChar[theOne])\n",
    "        first = second\n",
    "        second = temp\n",
    "generateText()\n",
    "text = ' '\n",
    "for i in range(len(theText)):\n",
    "    text = text + asciiToChars(theText[i])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser ney smoothing\n",
    "Kneser ney smoothing actually claimed to be the best language model (except KenLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateKNTriGram():\n",
    "    # D = [0.70, 0.20, 0.10]\n",
    "    D = 0.75\n",
    "    charCounts = dimension\n",
    "    bi_gram_count = getBigram(triGram)\n",
    "    uni_gram_count = getUnigram(bi_gram_count)\n",
    "    uni_gram = (uni_gram_count / sum(uni_gram_count))\n",
    "    bi_gram = np.zeros((dimension,dimension))\n",
    "    tri_gram = np.zeros((dimension,dimension,dimension))\n",
    "    for i in range(dimension):\n",
    "        for j in range(dimension):\n",
    "            if bi_gram_count[i][j] > 0:\n",
    "                bi_gram[i][j] = ( (bi_gram_count[i][j] - D) / uni_gram_count[i] ) + (((D * charCounts) / uni_gram_count[i]) * uni_gram[j])\n",
    "            else:\n",
    "                bi_gram[i][j] = (((D * charCounts) / uni_gram_count[i]) * uni_gram[j])\n",
    "                # print(bi_gram)\n",
    "    for w1 in range(dimension):\n",
    "        for w2 in range(dimension):\n",
    "            for w3 in range(dimension):\n",
    "                if triGram[w1][w2][w3] > 0:\n",
    "                    tri_gram[w1][w2][w3] = ((triGram[w1][w2][w3] - D) / bi_gram_count[w1][w2]) + ((D * charCounts) / bi_gram_count[w1][w2]) * bi_gram[w2][w3]\n",
    "                elif bi_gram_count[w1][w2] > 0:\n",
    "                    tri_gram[w1][w2][w3] = ((D * charCounts) / bi_gram_count[w1][w2]) * bi_gram[w2][w3]\n",
    "#                 else:\n",
    "#                     tri_gram[w1][w2][w3] = \n",
    "                    ##################### Work in pending #####################\n",
    "    print(tri_gram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network:\n",
    "Helper algorithms are in the first cell. Second cell has training and generation model. word2vec vectors are used and the number of epochs are set as 33. This number seemed to have a better generation and to avoid moving further to prevent overfitting. Keras is used to complete the model faster.\n",
    "Sigmoid is used for activation and the movinng average is always kept over the root mean square hence the RMSprop algorithm. Adam can also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)\n",
    "\n",
    "\n",
    "def load_data(data_dir, seq_length):\n",
    "    data = open(data_dir, 'r').read()\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "\n",
    "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "    X = np.zeros((int(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((int(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
    "    for i in range(0, int(len(data)/seq_length)):\n",
    "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "\n",
    "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "    return X, y, VOCAB_SIZE, ix_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 156902 characters\n",
      "Vocabulary size: 85 characters\n",
      "]óVbb'DFF'qqSwwwo!!!!D!!(qqqqwwNNN(((SDD!((55D!!D(((''QJJqqqzzJ777777777º[[4-----VV6666KKK666KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66KK66K6K6K66\n",
      "\n",
      "Epoch: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 83s - loss: 3.1801    \n",
      "D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 84s - loss: 2.7785    \n",
      "\n",
      "o the the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor to the tor t\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 82s - loss: 2.3754    \n",
      "é the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the Commiss of the\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 79s - loss: 2.1376    \n",
      "3 the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 79s - loss: 1.9576    \n",
      "y the report the proprammen and the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of the report the porticis of the report the porticial of \n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 79s - loss: 1.8034    \n",
      "on the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission the Commission th\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 81s - loss: 1.6722    \n",
      ".\n",
      "The regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the regions of the\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 85s - loss: 1.5600    \n",
      "?\n",
      "The Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commission in the Commissi\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 98s - loss: 1.4607    \n",
      "[ the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commission is the Commissi\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 94s - loss: 1.3739    \n",
      "; the concerned of the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the structural policy on the st\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 89s - loss: 1.2972    \n",
      "port of the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions and the Commission and programme and regions a\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 80s - loss: 1.2252    \n",
      "[Commission is the first consider to the first comments of the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the report on the regions and the r\n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 80s - loss: 1.1615    \n",
      "änd to the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 80s - loss: 1.0979    \n",
      "and to the entremely in the position of the European Parliament and the requirements of the European Parliament and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the Commission and the \n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Epoch 1/1\n",
      "3138/3138 [==============================] - 83s - loss: 1.0371    \n",
      "zent in the fact that the Commissioner proposals for expertent the regions and the Commission and programme for the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the Committee on Economic and Monetary Affairs, the \n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Epoch 1/1\n",
      "2250/3138 [====================>.........] - ETA: 23s - loss: 0.9713"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7f35f338a174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nEpoch: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mnb_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGENERATE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "HIDDEN_DIM = 500\n",
    "SEQ_LENGTH = 50\n",
    "WEIGHTS = ''\n",
    "mode = 'train'\n",
    "GENERATE_LENGTH = 500\n",
    "LAYER_NUM = 2\n",
    "\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data('assignment1-data/training.en', SEQ_LENGTH)\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "  model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "generate_text(model, 500, VOCAB_SIZE, ix_to_char)\n",
    "if not WEIGHTS == '':\n",
    "  model.load_weights(WEIGHTS)\n",
    "  nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "else:\n",
    "  nb_epoch = 0\n",
    "if mode == 'train' or WEIGHTS == '':\n",
    "  while nb_epoch<33:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    if nb_epoch % 10 == 0:\n",
    "      model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "\n",
    "elif WEIGHTS == '':\n",
    "  model.load_weights(WEIGHTS)\n",
    "  generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "  print('\\n\\n')\n",
    "else:\n",
    "  print('\\n\\nNothing to do!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(lines, model=''):\n",
    "    doc_length = 0\n",
    "    total_log_sum = 0\n",
    "    line_perplexities = []\n",
    "    for line in lines:\n",
    "        line_perplexity = 0\n",
    "        for char_index in range(len(line) - 2):\n",
    "#             print(ord(line[char_index]))\n",
    "            probability = tri_gram_add_one[getasciiForSingleChar(line[char_index])][getasciiForSingleChar(line[char_index+1])][getasciiForSingleChar(line[char_index+2])]\n",
    "            if probability > 0:\n",
    "                probability = np.log2(probability)\n",
    "            line_perplexity += probability\n",
    "        line_perplexities.append(line_perplexity)\n",
    "        doc_length += len(line)\n",
    "    for line_perplexity in line_perplexities:\n",
    "        total_log_sum += line_perplexity\n",
    "    perplexity = 2 ** -((1/(total_log_sum-2))*doc_length)\n",
    "    print(perplexity)\n",
    "    \n",
    "    \n",
    "\n",
    "#     perplexity = 2 ** -((1/(sum3-2))*sum2)\n",
    "#     print perplexity\n",
    "#     return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1735456014\n"
     ]
    }
   ],
   "source": [
    "processed_test_lines = []\n",
    "list = readFile(\"test\")\n",
    "for l in list:\n",
    "    for line in l:\n",
    "        processed_test_lines.append(process_line(line))\n",
    "perplexity(processed_test_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARPA2matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Issues with smoothing algorithms:\n",
    "1. They are not worried about long distant relationships between the characters. For example certain characters (wn) might be conditional on precedinng characters (like wn-3 and wn-5) skipping few grams.\n",
    "2. It does not carry forward any context variable for a specific sentence or a paragraph.\n",
    "3. In words language models, the words are not semantically or contextually clustered. This has been resolved by word2vec vectors. Creating a language model over these vectors in a might give us lot of information about a sentence while generating sequences from the model.\n",
    "4. Also on multiple iterations, the LSTM model seems to have promising text generation. But the training is time consuming. But the model outputs perfect english text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
